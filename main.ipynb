{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT STATEMENTS ###\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from models import *\n",
    "#from configs import cfg\n",
    "from nltk.translate import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS ###\n",
    "\n",
    "# returns a pandas dataframe of the given file\n",
    "def load_data(fname):\n",
    "    return pd.read_csv(fname)\n",
    "\n",
    "\n",
    "def process_train_data(data):\n",
    "    # TODO: Input is a pandas DataFrame and return a numpy array (or a torch Tensor/ Variable)\n",
    "    # that has all features (including characters in one hot encoded form).\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "def train_valid_split(data, labels):\n",
    "    # TODO: Takes in train data and labels as numpy array (or a torch Tensor/ Variable) and\n",
    "    # splits it into training and validation data.\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    \n",
    "def process_test_data(data):\n",
    "    # TODO: Takes in pandas DataFrame and returns a numpy array (or a torch Tensor/ Variable)\n",
    "    # that has all input features. Note that test data does not contain any review so you don't\n",
    "    # have to worry about one hot encoding the data.\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "def pad_data(orig_data):\n",
    "    # TODO: Since you will be training in batches and training sample of each batch may have reviews\n",
    "    # of varying lengths, you will need to pad your data so that all samples have reviews of length\n",
    "    # equal to the longest review in a batch. You will pad all the sequences with <EOS> character \n",
    "    # representation in one hot encoding.\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def train(model, X_train, y_train, X_valid, y_valid, cfg):\n",
    "    # TODO: Train the model!\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    \n",
    "def generate(model, X_test, cfg):\n",
    "    # TODO: Given n rows in test data, generate a list of n strings, where each string is the review\n",
    "    # corresponding to each input row in test data.\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    \n",
    "def save_to_file(outputs, fname):\n",
    "    # TODO: Given the list of generated review outputs and output file name, save all these reviews to\n",
    "    # the file in .txt format.\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_data = load_data(train_data_fname) # Generating the pandas DataFrame\\ntest_data = load_data(test_data_fname) # Generating the pandas DataFrame\\ntrain_data, train_labels = process_train_data(train_data) # Converting DataFrame to numpy array\\nX_train, y_train, X_valid, y_valid = train_valid_split(train_data, train_labels) # Splitting the train data into train-valid data\\nX_test = process_test_data(test_data) # Converting DataFrame to numpy array\\n\\nmodel = baselineLSTM(cfg) # Replace this with model = <your model name>(cfg)\\nif cfg[\\'cuda\\']:\\n    computing_device = torch.device(\"cuda\")\\nelse:\\n    computing_device = torch.device(\"cpu\")\\nmodel.to(computing_device)\\n\\ntrain(model, X_train, y_train, X_valid, y_valid, cfg) # Train the model\\noutputs = generate(model, X_test, cfg) # Generate the outputs for test data\\nsave_to_file(outputs, out_fname) # Save the generated outputs to a file\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_fname = \"Beeradvocate_Train_Short.csv\"\n",
    "test_data_fname = \"Beeradvocate_Test_Short\"\n",
    "out_fname = \"Output_Reviews.txt\"\n",
    "\n",
    "'''\n",
    "train_data = load_data(train_data_fname) # Generating the pandas DataFrame\n",
    "test_data = load_data(test_data_fname) # Generating the pandas DataFrame\n",
    "train_data, train_labels = process_train_data(train_data) # Converting DataFrame to numpy array\n",
    "X_train, y_train, X_valid, y_valid = train_valid_split(train_data, train_labels) # Splitting the train data into train-valid data\n",
    "X_test = process_test_data(test_data) # Converting DataFrame to numpy array\n",
    "\n",
    "model = baselineLSTM(cfg) # Replace this with model = <your model name>(cfg)\n",
    "if cfg['cuda']:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "model.to(computing_device)\n",
    "\n",
    "train(model, X_train, y_train, X_valid, y_valid, cfg) # Train the model\n",
    "outputs = generate(model, X_test, cfg) # Generate the outputs for test data\n",
    "save_to_file(outputs, out_fname) # Save the generated outputs to a file\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1269291, 14)\n"
     ]
    }
   ],
   "source": [
    "# loads the train data\n",
    "dataset = load_data(\"Beeradvocate_Train.csv\")\n",
    "print (dataset.shape)\n",
    "\n",
    "# full data set has 1269291 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1269291, 3)\n"
     ]
    }
   ],
   "source": [
    "# selects the data that will be used\n",
    "dataset = dataset[['beer/style', 'review/overall', 'review/text']]\n",
    "print (dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1269000, 3)\n"
     ]
    }
   ],
   "source": [
    "# drops the rows with missing data\n",
    "dataset.replace('', np.nan, inplace = True)\n",
    "dataset.dropna(inplace = True)\n",
    "dataset.reset_index(drop = True, inplace = True)\n",
    "print (dataset.shape)\n",
    "\n",
    "# full data dropped 291 rows -- count = 1269000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A lot of foam. But a lot.\\tIn the smell some b...\n",
       "1       Dark red color, light beige foam, average.\\tIn...\n",
       "2       Almost totally black. Beige foam, quite compac...\n",
       "3       Golden yellow color. White, compact foam, quit...\n",
       "4       According to the website, the style for the Ca...\n",
       "5       Poured from the bottle into a Chimay goblet.\\t...\n",
       "6       22 oz bottle from \"Lifesource\" Salem. $3.95 Ni...\n",
       "7       Bottle says \"Malt beverage brewed with Ginger ...\n",
       "8       I'm not sure why I picked this up... I like gi...\n",
       "9       Poured from a 22oz bomber into my Drie Fontein...\n",
       "10      OK, so the only reason I bought this while sho...\n",
       "11      Notes from 6/24\\t\\tA: Bright golden glowing be...\n",
       "12      22 oz. bomber,\\t\\tA: Pours a clear yellow with...\n",
       "13      Is it possible for a bottle to be a novelty pr...\n",
       "14      Brown in color, somewhere between a porter and...\n",
       "15      Caldera presents yet another circumstance wher...\n",
       "16      More of a 'dry' than a lager, tasted at the 20...\n",
       "17      Pours a murky light brown with a 1 inch fizzy ...\n",
       "18      Faint sudsy head with some with some dissipati...\n",
       "19      A new arrival to the West TN area... \\t\\tPours...\n",
       "20      Sampled 10/30/11 - Transferring the notes.\\t\\t...\n",
       "21      This is my first rauchbier. \\tPours a burnt am...\n",
       "22      A: Pours a rich, ruby color, clear. Maybe a fi...\n",
       "23      Pours a mahogany color, rich, with a tan head....\n",
       "24      Pours light caramel brown with reddish highlig...\n",
       "25      Poured a slightly cloudy deep amber/red color ...\n",
       "26      Big thanks to N2168 for knocking this off my w...\n",
       "27      On tap last night at the Caldera Tap House.\\t\\...\n",
       "28      22 oz. Think I picked this up in Chicago Binny...\n",
       "29      Poured into a 1/2 liter stein a deep rich ambe...\n",
       "                              ...                        \n",
       "971     Clear, straw color with a fluffy white head th...\n",
       "972     A: Pours a light, brilliantly clear pale yello...\n",
       "973     12 ox bottle into Mug.\\t\\tThis beer pours a st...\n",
       "974     Pours light pale straw yellow with a white hea...\n",
       "975     Pours a light straw color and has somewhat of ...\n",
       "976     12 oz bottle poured into pint glass\\t\\tA: Clea...\n",
       "977     12.30.09 at The Ambassador.\\t\\tA: glad I could...\n",
       "978     Joining in with the fiance on a low carb diet ...\n",
       "979     Poured into a pint glass.\\t\\tAppearance- Darke...\n",
       "980     Pours a pale straw color, crisp white head, mi...\n",
       "981     Pours clear and quite pale from a 12 oz. bottl...\n",
       "982     Never mind me - I'm just doing the CAN CAN on ...\n",
       "983     This beer surprised me! Had it at a party at w...\n",
       "984     Poured from a brown 12 oz. bottle. Smell is mi...\n",
       "985     Poured from a 12 oz bott...that's a lie, I don...\n",
       "986     Amstel-lite, the home beer of the Feyenoord fo...\n",
       "987     The color is a typical pale yellow. There's li...\n",
       "988     12 oz can into a pilsener glass\\t\\tPours a cry...\n",
       "989     12oz bottle poured into a mug. 3.5% ABV? Serio...\n",
       "990     A - yellowish tint, very little head\\t\\tS - Sl...\n",
       "991     What more really needs to be said about this b...\n",
       "992     I was excited about this beer when it came out...\n",
       "993     12oz bottle\\t\\tA- Pours a golden yellow color ...\n",
       "994     12oz. bottle served in a pilsener glass. Poure...\n",
       "995     Out at a bar with a crappy beer selection with...\n",
       "996     Poured into Pint Glass. Not Bad for a Light Be...\n",
       "997     Back before I knew better, I drank Amstel Ligh...\n",
       "998     Had a few bottles of this during a very boring...\n",
       "999     Poured from a 12oz bottle into a pint glass. B...\n",
       "1000    A- super pale extra light golden, dense white ...\n",
       "Name: review/text, Length: 1001, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['review/text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans the data\n",
    "dataset['beer/style'] = dataset['beer/style'].str.lower()\n",
    "dataset['review/text'] = dataset['review/text'].str.lower()\n",
    "\n",
    "char_list = string.ascii_lowercase + string.digits + '!$\\'()*,-.:;? '\n",
    "\n",
    "# converts all whitespace (space, tabs, newlines, etc.) into spaces\n",
    "whitespace_regex = '[' + string.whitespace + ']'\n",
    "dataset['review/text'] = dataset['review/text'].str.replace(whitespace_regex, ' ', regex = True)\n",
    "\n",
    "# removes all invalid characters\n",
    "invalid_char_regex = '[^' + char_list + ']'\n",
    "dataset['review/text'] = dataset['review/text'].str.replace(invalid_char_regex, '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique beers: 104\n"
     ]
    }
   ],
   "source": [
    "# creates lists of beer style and characters for a one-hot encoding\n",
    "beer_list = dataset['beer/style'].unique()\n",
    "beer_to_index = dict(zip(beer_list, range(beer_list.shape[0])))\n",
    "char_to_index = dict(zip(list(char_list), range(len(char_list))))\n",
    "\n",
    "print (\"Number of unique beers: \" + str(beer_list.shape[0]))\n",
    "\n",
    "# full dataset: 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_beer_list = beer_list\n",
    "super_beer_to_index = beer_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the input features into a one-hot encoding\n",
    "# result is a numpy array of features (one-hot) and a parallel numpy array of reviews (text)\n",
    "num_data = dataset.shape[0]                   # number of data points in the dataset\n",
    "input_feature_length = beer_list.shape[0] + 1 # one-hot encoding of beer style + rating\n",
    "\n",
    "# converts the input features into a one-hot encoding\n",
    "input_features = np.zeros((num_data, input_feature_length))\n",
    "labels = dataset['review/text'].values\n",
    "\n",
    "for index, review in dataset.iterrows():\n",
    "    beer_style = review['beer/style']\n",
    "    beer_index = beer_to_index[beer_style]\n",
    "    score = review['review/overall']\n",
    "    \n",
    "    input_features[index][beer_index] = 1\n",
    "    input_features[index][-1] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the dataset into training and validation sets\n",
    "percent_training = 0.8\n",
    "\n",
    "training_last_index = int(percent_training * num_data)\n",
    "\n",
    "x_train = input_features[:training_last_index]\n",
    "y_train = labels[:training_last_index]\n",
    "\n",
    "x_valid = input_features[training_last_index:]\n",
    "y_valid = labels[training_last_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317323, 13)\n"
     ]
    }
   ],
   "source": [
    "# loads the test data\n",
    "dataset_test = load_data(\"Beeradvocate_Test.csv\")\n",
    "print (dataset_test.shape)\n",
    "\n",
    "# full dataset: (317323, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317323, 2)\n"
     ]
    }
   ],
   "source": [
    "# selects the data that will be used\n",
    "dataset_test = dataset_test[['beer/style', 'review/overall']]\n",
    "print (dataset_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans the data\n",
    "dataset_test['beer/style'] = dataset_test['beer/style'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the input features into a one-hot encoding\n",
    "# result is a numpy array of features (one-hot)\n",
    "num_data_test = dataset_test.shape[0]                   # number of data points in the dataset\n",
    "\n",
    "# converts the input features into a one-hot encoding\n",
    "x_test = np.zeros((num_data_test, input_feature_length))\n",
    "\n",
    "for index, review in dataset_test.iterrows():\n",
    "    beer_style = review['beer/style']\n",
    "    beer_index = beer_to_index[beer_style]\n",
    "    score = review['review/overall']\n",
    "    \n",
    "    x_test[index][beer_index] = 1\n",
    "    x_test[index][-1] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains the model\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    \n",
    "else:\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "\n",
    "model = bGRU(cfg).to(computing_device)\n",
    "optimizer = optim.Adam(model.parameters(), cfg['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stop_count = 0\n",
    "min_loss = 100\n",
    "    \n",
    "for epoch in range(cfg['epochs']):\n",
    "    start_index = 0\n",
    "    end_index = cfg['batch_size']\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    while start_index < len(x_train):\n",
    "        batch_x = x_train[start_index:end_index]\n",
    "        batch_y = y_train[start_index:end_index]\n",
    "        \n",
    "        # convert the reviews char -> index\n",
    "        indexed_reviews = []\n",
    "        \n",
    "        for review in batch_y:\n",
    "            l = [char_to_index[x] for x in review]\n",
    "            indexed_reviews.append(l)\n",
    "        \n",
    "        # pad the reviews\n",
    "        max_review = 0\n",
    "        for r in indexed_reviews:\n",
    "            if len(r) > max_review:\n",
    "                max_review = len(r)\n",
    "        \n",
    "        for i in len(indexed_reviews):\n",
    "            to_pad = max_review - len(indexed_reviews[i])\n",
    "            indexed_reviews[i] = [<SOS>] + indexed_reviews[i] + [<EOS>] + [<PAD>] * to_pad\n",
    "        \n",
    "        # concatenate reviews to input features        \n",
    "        final_batch_x = []\n",
    "        \n",
    "        for j in range(cfg['batch_size']):\n",
    "            review_list = []\n",
    "            \n",
    "            for i in range(max_review + 2):\n",
    "                one_hot = [0] * (len(char_list) + 3)\n",
    "                one_hot[indexed_review[i]] = 1\n",
    "                i_f = batch_x[j] + one_hot\n",
    "                review_list.append(i_f)\n",
    "                \n",
    "            final_batch_x.append(review_list)\n",
    "        \n",
    "        # converts input to tensors\n",
    "        final_batch_x = torch.from_numpy(np.array(final_batch_x)).to(computing_device)\n",
    "              \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(final_batch_x)\n",
    "        \n",
    "        # convert labels to digits\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    train_loss.append(np.mean(losses))\n",
    "    \n",
    "    # validate\n",
    "    start_index = 0\n",
    "    end_index = cfg['batch_size']\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        while start_index < len(x_valid):\n",
    "            batch_x = x_valid[start_index:end_index]\n",
    "            batch_y = y_valid[start_index:end_index]\n",
    "\n",
    "            # one-hot encode the reviews char -> index -> one-hot\n",
    "\n",
    "            # pad the reviews\n",
    "\n",
    "            # concatenate reviews to input features\n",
    "\n",
    "            # converts inputs (numpy arrays) to tensors\n",
    "            batch_x = torch.from_numpy(batch_x).to(computing_device)\n",
    "            batch_y = torch.from_numpy(batch_y).to(computing_device)\n",
    "\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # convert labels to digits\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            losses.append(loss)\n",
    "            \n",
    "\n",
    "    average_loss = np.mean(losses)\n",
    "    valid_loss.append(average_loss)\n",
    "\n",
    "    \n",
    "    if average_loss >= min_loss:\n",
    "        early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= 2:\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        early_stop_count = 0\n",
    "        min_loss = average_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
